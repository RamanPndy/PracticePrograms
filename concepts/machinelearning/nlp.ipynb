{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the tagged words dataset\n",
    "df = pd.read_csv('/Users/rpandey1/Downloads/tagged_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fulton</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>county</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grand</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jury</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word   tag\n",
       "0     the   DET\n",
       "1  fulton  NOUN\n",
       "2  county  NOUN\n",
       "3   grand   ADJ\n",
       "4    jury  NOUN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "# Convert the words and tags to lowercase to ignore case\n",
    "df['word'] = df['word'].str.lower()\n",
    "df['tag'] = df['tag'].str.upper()\n",
    "\n",
    "# Create a contingency table (cross-tabulation) of words and their PoS tags\n",
    "crosstab = pd.crosstab(df['word'], df['tag'], normalize='columns')\n",
    "\n",
    "# Get the probability of 'his' given 'PRON'\n",
    "probability = crosstab.loc['his', 'PRON']\n",
    "\n",
    "# Round the result to 3 decimal places\n",
    "probability_rounded = round(probability, 3)\n",
    "\n",
    "# Print the result\n",
    "print(probability_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PoS tag for 'saw' in the sentence is: VERB\n"
     ]
    }
   ],
   "source": [
    "# Convert the word column to lowercase for case insensitivity\n",
    "df['word'] = df['word'].str.lower()\n",
    "\n",
    "# Count the frequency of each PoS tag for each word\n",
    "word_pos_freq = df.groupby(['word', 'tag']).size().reset_index(name='count')\n",
    "\n",
    "# Define the sentence and preprocess it\n",
    "sentence = \"I saw him running away\"\n",
    "words_in_sentence = sentence.lower().split()  # Convert sentence to lowercase and split into words\n",
    "\n",
    "# Find the most common PoS tag for the word 'saw'\n",
    "saw_pos = word_pos_freq[word_pos_freq['word'] == 'saw']\n",
    "most_common_saw_pos = saw_pos.sort_values(by='count', ascending=False).iloc[0]['tag']\n",
    "\n",
    "print(f\"The PoS tag for 'saw' in the sentence is: {most_common_saw_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the sentence\n",
    "sentence = \"It was the best of times and it was the worst of times.\"\n",
    "\n",
    "# Process the sentence using spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Find the token 'best' and count its children\n",
    "best_token = None\n",
    "for token in doc:\n",
    "    if token.text.lower() == 'best':\n",
    "        best_token = token\n",
    "        break\n",
    "\n",
    "# Get the number of children of the 'best' token\n",
    "num_children = len(list(best_token.children))\n",
    "num_children\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of children for the token 'best' is: 2\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the sentence\n",
    "sentence = \"It was the best of times and it was the worst of times.\"\n",
    "\n",
    "# Process the sentence using spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Find the token 'best' and count its children\n",
    "best_token = None\n",
    "for token in doc:\n",
    "    if token.text.lower() == 'best':\n",
    "        best_token = token\n",
    "        break\n",
    "\n",
    "# Get the number of children of the 'best' token\n",
    "num_children = len(list(best_token.children))\n",
    "print(f\"The number of children for the token 'best' is: {num_children}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average stock price is: $93.26\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text corpus containing price data\n",
    "text = read_text_from_file(\"stock-market-data.txt\")\n",
    "\n",
    "# Process the text using Spacy NLP pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Initialize a list to store all the extracted money values\n",
    "money_values = []\n",
    "\n",
    "# Extract entities with the label 'MONEY'\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'MONEY':\n",
    "        # Remove the dollar sign and convert the value to a float\n",
    "        money_value = float(ent.text.replace('$', '').replace(',', ''))\n",
    "        money_values.append(money_value)\n",
    "\n",
    "# Calculate the average price value\n",
    "if money_values:\n",
    "    average_price = sum(money_values) / len(money_values)\n",
    "    print(f\"The average stock price is: ${average_price:.2f}\")\n",
    "else:\n",
    "    print(\"No money entities found in the text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_entities(text):\n",
    "    # Process the text using Spacy NLP pipeline\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract entities with the label 'DATE'\n",
    "    date_entities = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "    \n",
    "    return date_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_entities = extract_date_entities(text)\n",
    "\n",
    "if date_entities:\n",
    "    print(\"Extracted Date Entities:\")\n",
    "    for date in date_entities:\n",
    "        print(date)\n",
    "else:\n",
    "    print(\"No date entities found in the text.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
